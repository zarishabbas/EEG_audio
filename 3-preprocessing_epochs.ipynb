{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following processes are applied to both filtered and unfiltered data:\n",
    "1. Epochs of 2 seconds, with an overlap of 0.75 are extracted from the data.\n",
    "2. Epochs are standardized.\n",
    "3. Epochs that are bad (e.g. due to eye blinks) are removed.\n",
    "\n",
    "This data for both datasets is saved as \"semiclean\"\n",
    "\n",
    "Further processing is done to make the data (potentially) more suitable for classification.\n",
    "4. ICA is run on the data to remove artifacts.\n",
    "5. the data is recomposed from resulting components.\n",
    "\n",
    "Four sets of data are saved as fif files, and as pandas dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import mne\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from mne.utils import ProgressBar\n",
    "from mne.preprocessing import ICA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "matplotlib.use('Qt5Agg')\n",
    "import pickle\n",
    "from autoreject import get_rejection_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('dataset2/data/filtered-ref/*.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_unfiltered = glob.glob('dataset2/data/unfiltered-ref/*.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(files, filtertype):\n",
    "    n_files = len(files)\n",
    "    data_filtered = pd.DataFrame()\n",
    "    data_semifiltered = pd.DataFrame()\n",
    "    # Make filtered dataset\n",
    "    print(\"Filtering {n} files\".format(n=n_files))\n",
    "    # Initialize a progress bar.\n",
    "    progress = ProgressBar(n_files, mesg='Filtering')\n",
    "    for file in files:\n",
    "        raw = mne.io.read_raw_fif(file, preload=True, verbose='error')\n",
    "        #create epochs\n",
    "        events = mne.make_fixed_length_events(raw, id=1, duration=2, overlap=.75)\n",
    "        raw.info['projs'] = []\n",
    "        epochs = mne.Epochs(raw, events, tmin=0, tmax=2, baseline=(0, 0), detrend=1, reject_by_annotation=True, preload=True, verbose='error')\n",
    "        reject = get_rejection_threshold(epochs, decim=1)\n",
    "        reject['mag'] = 5e-12\n",
    "        reject['grad'] = 4000e-13\n",
    "        # standardize epochs\n",
    "        X = epochs.get_data() # EEG signals (n_epochs, n_channels, n_times)\n",
    "        y = epochs.events[:, 2] # class labels (n_epochs)\n",
    "        epochs_std = mne.decoding.Scaler(epochs.info, scalings='mean').fit_transform(X, y)\n",
    "        epochs_std = mne.EpochsArray(epochs_std, epochs.info, events=epochs.events)\n",
    "        # reject bad epochs\n",
    "        ar = AutoReject(verbose=True)\n",
    "        epochs_clean = ar.fit_transform(epochs_std)\n",
    "        \n",
    "        # save epochs\n",
    "        if not os.path.exists(f'dataset2/data/{filtertype}-semiclean'):\n",
    "            os.makedirs(f'dataset2/data/{filtertype}-semiclean')\n",
    "        epochs_clean.save(f'dataset2/data/{filtertype}-semiclean/{file.split(\"/\")[-1]}')\n",
    "\n",
    "        # save dataframe\n",
    "        temp_df = epochs_clean.to_data_frame()\n",
    "        temp_df['subject'] = f'{file.split(\"/\")[-1][0:11]}'\n",
    "        data_semifiltered = data_semifiltered.append(temp_df, ignore_index=True)\n",
    "\n",
    "        # aapply ICA\n",
    "        epochs_tmp = epochs_clean.copy()\n",
    "        picks = mne.pick_types(epochs_tmp.info, eeg=True, eog=False, stim=False, exclude='bads')\n",
    "        ica = mne.preprocessing.ICA(method=\"infomax\", fit_params=dict(extended=True), random_state=1, n_components = 19)\n",
    "        ica.fit(epochs_tmp, picks=picks, reject=reject)\n",
    "        ica.apply(epochs_tmp)\n",
    "        # save in folder\n",
    "\n",
    "        if not os.path.exists(f'dataset2/data/{filtertype}-clean'):\n",
    "            os.makedirs(f'dataset2/data/{filtertype}-clean')\n",
    "        epochs_tmp.save(f'dataset2/data/{filtertype}-clean/{file.split(\"/\")[-1]}')\n",
    "\n",
    "        #save in dataframe\n",
    "        temp_df = epochs_tmp.to_data_frame()\n",
    "        temp_df['subject'] = f'{file.split(\"/\")[-1][0:11]}'\n",
    "        data_filtered = data_filtered.append(temp_df, ignore_index=True)\n",
    "        progress.update_with_increment_value(1)\n",
    "    \n",
    "    #pickle data filtered\n",
    "    with open(f'dataset2/data/{filtertype}_filtered_epoch.pkl', 'wb') as f:\n",
    "        pickle.dump(data_filtered, f)\n",
    "    #pickle data semifiltered\n",
    "    with open(f'dataset2/data/{filtertype}_semifiltered_epoch.pkl', 'wb') as f:\n",
    "        pickle.dump(data_filtered, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(files_unfiltered, 'unfiltered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(files, 'filtered')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ffc1d896e1bfad38c5d10b4cb188f760e098077a43cba5553fb663d8bbed5c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('mne2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
